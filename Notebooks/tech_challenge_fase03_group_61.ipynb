{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tech Challenge - Group 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 03\n",
    "\n",
    "# Daniel Tavarez\n",
    "# Matheus Diniz\n",
    "# Miguel Chiarello\n",
    "# Taynara Nascimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apache Spark Library\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import col, expr, lit, substring, concat, concat_ws, when, coalesce\n",
    "from pyspark.sql import functions as F # for more sql functions\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
    "\n",
    "## Spark Sessions\n",
    "# spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "# spark = SparkSession.builder.appName(\"Our First Spark Example\").getOrCreate()\n",
    "# spark = SparkSession.builder.appName(\"spark-facility\").config(\"spark.sql.broadcastTimeout\", \"100000\").config('spark.sql.autoBroadcastJoinThreshold', '-1').enableHiveSupport().getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "## COLAB SETTINGS\n",
    "# !sudo apt update\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "# !wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "# !tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "\n",
    "\n",
    "## Docs\n",
    "# https://www.pola.rs/\n",
    "# https://pandas.pydata.org/docs/reference/index.html\n",
    "# https://numpy.org/numpy-tutorials/\n",
    "\n",
    "# https://matplotlib.org/stable/plot_types/index.html\n",
    "# https://scikit-learn.org/stable/modules/classes.html\n",
    "# https://www.tensorflow.org/api_docs/python/tf\n",
    "# https://pytorch.org/get-started/locally/\n",
    "\n",
    "\n",
    "## Tech Challenge Reference\n",
    "# IBGE Microdados: https://www.ibge.gov.br/estatisticas/investigacoes-experimentais/estatisticas-experimentais/27946-divulgacao-semanal-pnadcovid1?t=microdados&utm_source=covid19&utm_medium=hotsite&utm_campaign=covid_19\n",
    "# Microdados Source: https://www.ibge.gov.br/estatisticas/downloads-estatisticas.html?caminho=Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_PNAD_COVID19/Microdados/Dados\n",
    "# Dicion√°rio Source: https://www.ibge.gov.br/estatisticas/downloads-estatisticas.html?caminho=Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_PNAD_COVID19/Microdados/Documentacao\n",
    "# Streamlite: https://hospitalytics.streamlit.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Analysis Library \n",
    "# import polars as pl\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "## DataViz Library \n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "## Machine Learning Library \n",
    "# from sklearn import datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## Aditional Libraries \n",
    "# import math as mt\n",
    "# import folium as fl\n",
    "# from folium import plugins\n",
    "# from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_pd_09.columns: Index(['Ano', 'UF', 'CAPITAL', 'RM_RIDE', 'V1008', 'V1012', 'V1013', 'V1016',\n",
      "       'Estrato', 'UPA',\n",
      "       ...\n",
      "       'F001', 'F0021', 'F0022', 'F002A1', 'F002A2', 'F002A3', 'F002A4',\n",
      "       'F002A5', 'F0061', 'F006'],\n",
      "      dtype='object', length=145)\n",
      "db_pd_09.columns_count : 145\n",
      "db_pd_10.columns: Index(['Ano', 'UF', 'CAPITAL', 'RM_RIDE', 'V1008', 'V1012', 'V1013', 'V1016',\n",
      "       'Estrato', 'UPA',\n",
      "       ...\n",
      "       'F001', 'F0021', 'F0022', 'F002A1', 'F002A2', 'F002A3', 'F002A4',\n",
      "       'F002A5', 'F0061', 'F006'],\n",
      "      dtype='object', length=145)\n",
      "db_pd_10.columns_count : 145\n",
      "db_pd_11.columns: Index(['Ano', 'UF', 'CAPITAL', 'RM_RIDE', 'V1008', 'V1012', 'V1013', 'V1016',\n",
      "       'Estrato', 'UPA',\n",
      "       ...\n",
      "       'F001', 'F0021', 'F0022', 'F002A1', 'F002A2', 'F002A3', 'F002A4',\n",
      "       'F002A5', 'F0061', 'F006'],\n",
      "      dtype='object', length=148)\n",
      "db_pd_11.columns_count : 148\n"
     ]
    }
   ],
   "source": [
    "# CSV Local Sources\n",
    "\n",
    "# db_pl = pl.read_csv(\n",
    "#     r'..\\Source\\PNAD_COVID_112020\\PNAD_COVID_112020.csv'\n",
    "#     ,separator=','\n",
    "# )\n",
    "\n",
    "db_pd_09 = pd.read_csv(\n",
    "    r'..\\Source\\PNAD_COVID_092020\\PNAD_COVID_092020.csv'\n",
    "    , sep=\",\"\n",
    ")\n",
    "\n",
    "db_pd_10 = pd.read_csv(\n",
    "    r'..\\Source\\PNAD_COVID_102020\\PNAD_COVID_102020.csv'\n",
    "    , sep=\",\"\n",
    ")\n",
    "\n",
    "db_pd_11 = pd.read_csv(\n",
    "    r'..\\Source\\PNAD_COVID_112020\\PNAD_COVID_112020.csv'\n",
    "    , sep=\",\"\n",
    ")\n",
    "\n",
    "print('db_pd_09.columns:', db_pd_09.columns)\n",
    "print('db_pd_09.columns_count :', len(db_pd_09.columns))\n",
    "\n",
    "print('db_pd_10.columns:', db_pd_10.columns)\n",
    "print('db_pd_10.columns_count :', len(db_pd_10.columns))\n",
    "\n",
    "print('db_pd_11.columns:', db_pd_11.columns)\n",
    "print('db_pd_11.columns_count :', len(db_pd_11.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if spark is initialized\n",
    "# df = spark.sql('''select 'Sucesso total, estamos online!' as hello''')\n",
    "# df.show()\n",
    "\n",
    "\n",
    "# df = spark.read.csv(r'..\\Source\\PNAD_COVID_092020\\PNAD_COVID_092020.csv', sep = ',', inferSchema = True, header = True)\n",
    "\n",
    "# print('df.count  :', df.count())\n",
    "# print('df.col ct :', len(df.columns))\n",
    "# print('df.columns:', df.columns)\n",
    "\n",
    "\n",
    "\n",
    "# db_pd = db_pd_09.append(db_pd_10, ignore_index=True)\n",
    "# db_pd = db_pd.append(db_pd_11, ignore_index=True)\n",
    "\n",
    "# db_pd.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
