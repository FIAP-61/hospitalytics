{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a81634-de5f-4efd-b19c-828bfe7fc374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Read CSV files\n",
    "df_09 = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(\"/FileStore/tables/PNAD_COVID_092020.csv\")\n",
    "\n",
    "df_10 = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(\"/FileStore/tables/PNAD_COVID_102020.csv\")\n",
    "\n",
    "df_11 = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(\"/FileStore/tables/PNAD_COVID_112020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a4e775-906a-4ad9-a385-f54e663923ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_09 = df_09.dropDuplicates()\n",
    "df_10 = df_10.dropDuplicates()\n",
    "df_11 = df_11.dropDuplicates()\n",
    "\n",
    "# Exclude df_11 exclusive columns\n",
    "df_11 = df_11.drop(*['A006A','A006B','A007A'])\n",
    "\n",
    "# Appending dfs into one df\n",
    "df_pnad = df_09.union(df_10)\n",
    "df_pnad = df_pnad.union(df_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4586af1-a028-4f43-bd21-e272b9d39a7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter columns\n",
    "db = df_pnad.select(\n",
    "    F.col(\"Ano\").alias(\"ano\"),\n",
    "    F.col(\"UF\").alias(\"uf\"),\n",
    "    F.col(\"CAPITAL\").alias(\"capital\"),\n",
    "    F.col(\"RM_RIDE\").alias(\"rm_ride\"),\n",
    "    F.col(\"V1008\").alias(\"n_domicilio\"),\n",
    "    F.col(\"V1012\").alias(\"n_semana\"),\n",
    "    F.col(\"V1013\").alias(\"mes_pesquisa\"),\n",
    "    F.col(\"V1016\").alias(\"n_entrevista_domicilio\"),\n",
    "    F.col(\"Estrato\").alias(\"estrato\"),\n",
    "    F.col(\"UPA\").alias(\"unidade_primaria_amostragem\"),\n",
    "    F.col(\"V1022\").alias(\"situacao_domicilio\"),\n",
    "    F.col(\"V1023\").alias(\"tipo_area\"),\n",
    "    F.col(\"V1030\").alias(\"projecao_populacao\"),\n",
    "    F.col(\"V1031\").alias(\"peso_domicilio_pessoas_com_pos\"),\n",
    "    F.col(\"V1032\").alias(\"peso_domicilio_pessoas_sem_pos\"),\n",
    "    F.col(\"posest\").alias(\"dominios_projecao\"),\n",
    "\n",
    "    # Parte A - Características gerais dos moradores\n",
    "    F.col(\"A002\").alias(\"idade_morador\"),  # 1\n",
    "    F.col(\"A003\").alias(\"sexo\"),           # 2\n",
    "\n",
    "    # Parte B - COVID19 - Todos os moradores\n",
    "    F.col(\"B0031\").alias(\"ficou_em_casa\"),                     # 3\n",
    "    F.col(\"B0041\").alias(\"buscou_atendimento_posto_ubs_esf\"),  # 4\n",
    "    F.col(\"B0042\").alias(\"buscou_atendimento_ps_sus_upa\"),     # 5\n",
    "    F.col(\"B0043\").alias(\"buscou_atendimento_hospital_sus\"),   # 6\n",
    "    F.col(\"B005\").alias(\"ficou_internado\"),                    # 7\n",
    "    F.col(\"B006\").alias(\"internado_risco\"),                    # 8\n",
    "    F.col(\"B007\").alias(\"possui_plano_saude\"),                 # 9\n",
    "\n",
    "    # Parte C - Características de trabalho das pessoas de 14 anos ou mais de idade\n",
    "    F.col(\"C011A\").alias(\"respondeu_valor_recebido\"),          # 10\n",
    "    F.col(\"C011A1\").alias(\"valores_recebidos_unidade\"),        # 10\n",
    "    F.col(\"C011A11\").alias(\"valores_recebidos_faixa\"),         # 10\n",
    "    F.col(\"C011A12\").alias(\"valores_recebidos_reais\"),         # 10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a table\n",
    "db.write.format(\"parquet\").saveAsTable(\"pnad_covid\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 59063656447379,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "create_db_table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
